package monitoring

import (
	"context"
	"fmt"
	"log"
	"os/exec"
	"sync"
	"time"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)

// NeuralAutoHealingSystem syst√®me d'auto-gu√©rison intelligent
type NeuralAutoHealingSystem struct {
	// M√©triques pour l'auto-healing
	autoHealingAttempts   *prometheus.CounterVec
	autoHealingSuccesses  *prometheus.CounterVec
	autoHealingFailures   *prometheus.CounterVec
	escalationCounter     *prometheus.CounterVec
	
	// Configuration
	maxRetries           int
	retryDelay           time.Duration
	escalationThreshold  int
	recoveryStrategies   map[string][]RecoveryStrategy
	
	// √âtat interne
	serviceFailures      map[string]*ServiceFailureTracker
	autonomyManager      AdvancedAutonomyManager
	notificationSystem   NotificationSystem
	mutex                sync.RWMutex
	isActive             bool
}

// RecoveryStrategy repr√©sente une strat√©gie de r√©cup√©ration
type RecoveryStrategy struct {
	Name        string
	Priority    int
	Action      RecoveryAction
	Timeout     time.Duration
	Conditions  []RecoveryCondition
}

// RecoveryAction d√©finit une action de r√©cup√©ration
type RecoveryAction func(ctx context.Context, service string, failure *ServiceFailureTracker) error

// RecoveryCondition d√©finit une condition pour appliquer une strat√©gie
type RecoveryCondition func(service string, failure *ServiceFailureTracker) bool

// ServiceFailureTracker suit les √©checs d'un service
type ServiceFailureTracker struct {
	Service           string
	FailureCount      int
	FirstFailure      time.Time
	LastFailure       time.Time
	ConsecutiveFailures int
	LastRecoveryAttempt time.Time
	RecoveryAttempts    []RecoveryAttempt
	CurrentStatus       ServiceStatus
}

// RecoveryAttempt repr√©sente une tentative de r√©cup√©ration
type RecoveryAttempt struct {
	Timestamp  time.Time
	Strategy   string
	Success    bool
	Error      error
	Duration   time.Duration
}

// ServiceStatus √©num√©ration pour l'√©tat du service
type ServiceStatus int

const (
	StatusUnknown ServiceStatus = iota
	StatusHealthy
	StatusDegraded
	StatusFailed
	StatusRecovering
)

// AdvancedAutonomyManager interface pour l'escalade vers le syst√®me d'autonomie
type AdvancedAutonomyManager interface {
	HandleServiceFailure(ctx context.Context, service string, failure *ServiceFailureTracker) error
	NotifyEscalation(service string, reason string) error
}

// NotificationSystem interface pour les notifications
type NotificationSystem interface {
	SendAlert(level string, service string, message string) error
	LogEvent(event string, details map[string]interface{}) error
}

// NewNeuralAutoHealingSystem cr√©e une nouvelle instance du syst√®me d'auto-healing
func NewNeuralAutoHealingSystem(autonomyManager AdvancedAutonomyManager, notifications NotificationSystem) *NeuralAutoHealingSystem {
	nahs := &NeuralAutoHealingSystem{
		maxRetries:          3,
		retryDelay:          30 * time.Second,
		escalationThreshold: 5,
		serviceFailures:     make(map[string]*ServiceFailureTracker),
		autonomyManager:     autonomyManager,
		notificationSystem:  notifications,
		recoveryStrategies:  make(map[string][]RecoveryStrategy),
	}

	// Initialisation des m√©triques Prometheus
	nahs.autoHealingAttempts = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Namespace: "smart_infrastructure",
			Subsystem: "auto_healing",
			Name:      "attempts_total",
			Help:      "Total number of auto-healing attempts",
		},
		[]string{"service", "strategy"},
	)

	nahs.autoHealingSuccesses = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Namespace: "smart_infrastructure",
			Subsystem: "auto_healing",
			Name:      "successes_total",
			Help:      "Total number of successful auto-healing attempts",
		},
		[]string{"service", "strategy"},
	)

	nahs.autoHealingFailures = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Namespace: "smart_infrastructure",
			Subsystem: "auto_healing",
			Name:      "failures_total",
			Help:      "Total number of failed auto-healing attempts",
		},
		[]string{"service", "strategy"},
	)

	nahs.escalationCounter = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Namespace: "smart_infrastructure",
			Subsystem: "auto_healing",
			Name:      "escalations_total",
			Help:      "Total number of escalations to AdvancedAutonomyManager",
		},
		[]string{"service", "reason"},
	)

	// Configuration des strat√©gies de r√©cup√©ration par d√©faut
	nahs.setupDefaultRecoveryStrategies()

	return nahs
}

// setupDefaultRecoveryStrategies configure les strat√©gies de r√©cup√©ration par d√©faut
func (nahs *NeuralAutoHealingSystem) setupDefaultRecoveryStrategies() {
	// Strat√©gies pour QDrant
	nahs.recoveryStrategies["qdrant"] = []RecoveryStrategy{
		{
			Name:     "qdrant_simple_restart",
			Priority: 1,
			Action:   nahs.dockerRestartAction,
			Timeout:  60 * time.Second,
			Conditions: []RecoveryCondition{
				nahs.conditionConsecutiveFailuresBelow(3),
			},
		},
		{
			Name:     "qdrant_force_restart",
			Priority: 2,
			Action:   nahs.dockerForceRestartAction,
			Timeout:  120 * time.Second,
			Conditions: []RecoveryCondition{
				nahs.conditionConsecutiveFailuresAbove(2),
			},
		},
		{
			Name:     "qdrant_recreate_container",
			Priority: 3,
			Action:   nahs.dockerRecreateAction,
			Timeout:  180 * time.Second,
			Conditions: []RecoveryCondition{
				nahs.conditionConsecutiveFailuresAbove(5),
			},
		},
	}

	// Strat√©gies pour Redis
	nahs.recoveryStrategies["redis"] = []RecoveryStrategy{
		{
			Name:     "redis_simple_restart",
			Priority: 1,
			Action:   nahs.dockerRestartAction,
			Timeout:  30 * time.Second,
			Conditions: []RecoveryCondition{
				nahs.conditionConsecutiveFailuresBelow(3),
			},
		},
		{
			Name:     "redis_flush_and_restart",
			Priority: 2,
			Action:   nahs.redisFlushAndRestartAction,
			Timeout:  60 * time.Second,
			Conditions: []RecoveryCondition{
				nahs.conditionConsecutiveFailuresAbove(2),
			},
		},
	}

	// Strat√©gies pour RAG Server
	nahs.recoveryStrategies["rag-server"] = []RecoveryStrategy{
		{
			Name:     "rag_server_restart",
			Priority: 1,
			Action:   nahs.dockerRestartAction,
			Timeout:  90 * time.Second,
		},
		{
			Name:     "rag_server_rebuild",
			Priority: 2,
			Action:   nahs.ragServerRebuildAction,
			Timeout:  300 * time.Second,
			Conditions: []RecoveryCondition{
				nahs.conditionConsecutiveFailuresAbove(3),
			},
		},
	}

	// Strat√©gies pour Prometheus
	nahs.recoveryStrategies["prometheus"] = []RecoveryStrategy{
		{
			Name:     "prometheus_restart",
			Priority: 1,
			Action:   nahs.dockerRestartAction,
			Timeout:  45 * time.Second,
		},
		{
			Name:     "prometheus_config_reload",
			Priority: 2,
			Action:   nahs.prometheusReloadConfigAction,
			Timeout:  30 * time.Second,
		},
	}

	// Strat√©gies pour Grafana
	nahs.recoveryStrategies["grafana"] = []RecoveryStrategy{
		{
			Name:     "grafana_restart",
			Priority: 1,
			Action:   nahs.dockerRestartAction,
			Timeout:  60 * time.Second,
		},
	}
}

// DetectAndHeal d√©tecte les pannes et lance le processus d'auto-healing
func (nahs *NeuralAutoHealingSystem) DetectAndHeal(ctx context.Context, healthStatuses map[string]ServiceHealthStatus) error {
	nahs.mutex.Lock()
	defer nahs.mutex.Unlock()

	log.Println("üîß Neural Auto-Healing System: Analyzing service health...")

	for serviceName, status := range healthStatuses {
		if !status.Healthy {
			log.Printf("‚ö†Ô∏è  Service failure detected: %s", serviceName)
			
			// Mettre √† jour ou cr√©er le tracker de failure
			tracker := nahs.updateFailureTracker(serviceName, status)
			
			// D√©cider si on doit tenter une r√©cup√©ration
			if nahs.shouldAttemptRecovery(tracker) {
				go nahs.attemptRecovery(ctx, serviceName, tracker)
			} else if nahs.shouldEscalate(tracker) {
				go nahs.escalateToAutonomyManager(ctx, serviceName, tracker)
			}
		} else {
			// Service is healthy, reset failure tracker if it exists
			if tracker, exists := nahs.serviceFailures[serviceName]; exists {
				tracker.CurrentStatus = StatusHealthy
				tracker.ConsecutiveFailures = 0
				log.Printf("‚úÖ Service %s recovered", serviceName)
			}
		}
	}

	return nil
}

// updateFailureTracker met √† jour le tracker de failure pour un service
func (nahs *NeuralAutoHealingSystem) updateFailureTracker(serviceName string, status ServiceHealthStatus) *ServiceFailureTracker {
	tracker, exists := nahs.serviceFailures[serviceName]
	if !exists {
		tracker = &ServiceFailureTracker{
			Service:         serviceName,
			FirstFailure:    time.Now(),
			RecoveryAttempts: []RecoveryAttempt{},
		}
		nahs.serviceFailures[serviceName] = tracker
	}

	tracker.FailureCount++
	tracker.ConsecutiveFailures++
	tracker.LastFailure = time.Now()
	tracker.CurrentStatus = StatusFailed

	return tracker
}

// shouldAttemptRecovery d√©termine si on doit tenter une r√©cup√©ration
func (nahs *NeuralAutoHealingSystem) shouldAttemptRecovery(tracker *ServiceFailureTracker) bool {
	// Ne pas tenter si on a d√©j√† essay√© r√©cemment
	if time.Since(tracker.LastRecoveryAttempt) < nahs.retryDelay {
		return false
	}

	// Ne pas tenter si on a d√©pass√© le nombre max de tentatives
	if len(tracker.RecoveryAttempts) >= nahs.maxRetries {
		return false
	}

	return true
}

// shouldEscalate d√©termine si on doit escalader vers l'AdvancedAutonomyManager
func (nahs *NeuralAutoHealingSystem) shouldEscalate(tracker *ServiceFailureTracker) bool {
	return tracker.ConsecutiveFailures >= nahs.escalationThreshold
}

// attemptRecovery tente la r√©cup√©ration d'un service
func (nahs *NeuralAutoHealingSystem) attemptRecovery(ctx context.Context, serviceName string, tracker *ServiceFailureTracker) {
	log.Printf("üîÑ Attempting recovery for service: %s", serviceName)
	
	tracker.CurrentStatus = StatusRecovering
	tracker.LastRecoveryAttempt = time.Now()

	strategies, exists := nahs.recoveryStrategies[serviceName]
	if !exists {
		log.Printf("‚ö†Ô∏è  No recovery strategies defined for service: %s", serviceName)
		return
	}

	// Essayer les strat√©gies par ordre de priorit√©
	for _, strategy := range strategies {
		if nahs.evaluateConditions(strategy.Conditions, serviceName, tracker) {
			log.Printf("üéØ Applying recovery strategy: %s for %s", strategy.Name, serviceName)
			
			attempt := RecoveryAttempt{
				Timestamp: time.Now(),
				Strategy:  strategy.Name,
			}

			// Cr√©er un contexte avec timeout pour la strat√©gie
			strategyCtx, cancel := context.WithTimeout(ctx, strategy.Timeout)
			defer cancel()

			// Ex√©cuter la strat√©gie
			start := time.Now()
			err := strategy.Action(strategyCtx, serviceName, tracker)
			attempt.Duration = time.Since(start)
			attempt.Error = err
			attempt.Success = err == nil

			// Enregistrer la tentative
			tracker.RecoveryAttempts = append(tracker.RecoveryAttempts, attempt)

			// Mettre √† jour les m√©triques
			nahs.autoHealingAttempts.WithLabelValues(serviceName, strategy.Name).Inc()
			
			if attempt.Success {
				nahs.autoHealingSuccesses.WithLabelValues(serviceName, strategy.Name).Inc()
				log.Printf("‚úÖ Recovery successful for %s using strategy %s", serviceName, strategy.Name)
				
				// Notifier le succ√®s
				nahs.notificationSystem.SendAlert("info", serviceName, 
					fmt.Sprintf("Service recovered using strategy: %s", strategy.Name))
				
				tracker.CurrentStatus = StatusHealthy
				return
			} else {
				nahs.autoHealingFailures.WithLabelValues(serviceName, strategy.Name).Inc()
				log.Printf("‚ùå Recovery failed for %s using strategy %s: %v", serviceName, strategy.Name, err)
			}
		}
	}

	// Toutes les strat√©gies ont √©chou√©
	log.Printf("‚ùå All recovery strategies failed for service: %s", serviceName)
	nahs.notificationSystem.SendAlert("error", serviceName, "All recovery strategies failed")
}

// evaluateConditions √©value si les conditions d'une strat√©gie sont remplies
func (nahs *NeuralAutoHealingSystem) evaluateConditions(conditions []RecoveryCondition, serviceName string, tracker *ServiceFailureTracker) bool {
	for _, condition := range conditions {
		if !condition(serviceName, tracker) {
			return false
		}
	}
	return true
}

// escalateToAutonomyManager escalade vers l'AdvancedAutonomyManager
func (nahs *NeuralAutoHealingSystem) escalateToAutonomyManager(ctx context.Context, serviceName string, tracker *ServiceFailureTracker) {
	log.Printf("üö® Escalating service failure to AdvancedAutonomyManager: %s", serviceName)
	
	reason := fmt.Sprintf("Consecutive failures: %d, Total attempts: %d", 
		tracker.ConsecutiveFailures, len(tracker.RecoveryAttempts))
	
	nahs.escalationCounter.WithLabelValues(serviceName, "max_retries_exceeded").Inc()
	
	err := nahs.autonomyManager.HandleServiceFailure(ctx, serviceName, tracker)
	if err != nil {
		log.Printf("‚ùå Escalation failed: %v", err)
		nahs.notificationSystem.SendAlert("critical", serviceName, 
			fmt.Sprintf("Escalation failed: %v", err))
	} else {
		nahs.autonomyManager.NotifyEscalation(serviceName, reason)
		nahs.notificationSystem.SendAlert("warning", serviceName, 
			fmt.Sprintf("Escalated to AdvancedAutonomyManager: %s", reason))
	}
}

// Actions de r√©cup√©ration sp√©cifiques

// dockerRestartAction red√©marre un conteneur Docker
func (nahs *NeuralAutoHealingSystem) dockerRestartAction(ctx context.Context, service string, tracker *ServiceFailureTracker) error {
	log.Printf("üîÑ Docker restart action for service: %s", service)
	
	cmd := exec.CommandContext(ctx, "docker-compose", "restart", service)
	output, err := cmd.CombinedOutput()
	
	if err != nil {
		return fmt.Errorf("docker restart failed: %w, output: %s", err, string(output))
	}
	
	// Attendre un peu pour que le service red√©marre
	time.Sleep(10 * time.Second)
	return nil
}

// dockerForceRestartAction force le red√©marrage d'un conteneur Docker
func (nahs *NeuralAutoHealingSystem) dockerForceRestartAction(ctx context.Context, service string, tracker *ServiceFailureTracker) error {
	log.Printf("üîÑ Docker force restart action for service: %s", service)
	
	// Stop forc√©
	stopCmd := exec.CommandContext(ctx, "docker-compose", "kill", service)
	stopCmd.Run() // Ignorer les erreurs
	
	// Red√©marrage
	startCmd := exec.CommandContext(ctx, "docker-compose", "up", "-d", service)
	output, err := startCmd.CombinedOutput()
	
	if err != nil {
		return fmt.Errorf("docker force restart failed: %w, output: %s", err, string(output))
	}
	
	time.Sleep(15 * time.Second)
	return nil
}

// dockerRecreateAction recr√©e compl√®tement un conteneur Docker
func (nahs *NeuralAutoHealingSystem) dockerRecreateAction(ctx context.Context, service string, tracker *ServiceFailureTracker) error {
	log.Printf("üîÑ Docker recreate action for service: %s", service)
	
	// Down
	downCmd := exec.CommandContext(ctx, "docker-compose", "down", service)
	downCmd.Run()
	
	// Up avec force recreate
	upCmd := exec.CommandContext(ctx, "docker-compose", "up", "-d", "--force-recreate", service)
	output, err := upCmd.CombinedOutput()
	
	if err != nil {
		return fmt.Errorf("docker recreate failed: %w, output: %s", err, string(output))
	}
	
	time.Sleep(30 * time.Second)
	return nil
}

// redisFlushAndRestartAction vide Redis et le red√©marre
func (nahs *NeuralAutoHealingSystem) redisFlushAndRestartAction(ctx context.Context, service string, tracker *ServiceFailureTracker) error {
	log.Printf("üîÑ Redis flush and restart action for service: %s", service)
	
	// Essayer de vider Redis avant red√©marrage
	flushCmd := exec.CommandContext(ctx, "docker", "exec", "redis", "redis-cli", "FLUSHALL")
	flushCmd.Run() // Ignorer les erreurs
	
	return nahs.dockerRestartAction(ctx, service, tracker)
}

// ragServerRebuildAction reconstruit et red√©marre le serveur RAG
func (nahs *NeuralAutoHealingSystem) ragServerRebuildAction(ctx context.Context, service string, tracker *ServiceFailureTracker) error {
	log.Printf("üîÑ RAG Server rebuild action for service: %s", service)
	
	// Stop
	stopCmd := exec.CommandContext(ctx, "docker-compose", "stop", service)
	stopCmd.Run()
	
	// Build avec no-cache
	buildCmd := exec.CommandContext(ctx, "docker-compose", "build", "--no-cache", service)
	output, err := buildCmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("rag server build failed: %w, output: %s", err, string(output))
	}
	
	// Start
	startCmd := exec.CommandContext(ctx, "docker-compose", "up", "-d", service)
	output, err = startCmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("rag server start failed: %w, output: %s", err, string(output))
	}
	
	time.Sleep(60 * time.Second) // RAG server needs more time to start
	return nil
}

// prometheusReloadConfigAction recharge la configuration Prometheus
func (nahs *NeuralAutoHealingSystem) prometheusReloadConfigAction(ctx context.Context, service string, tracker *ServiceFailureTracker) error {
	log.Printf("üîÑ Prometheus config reload action for service: %s", service)
	
	// Essayer de recharger la config via l'API
	reloadCmd := exec.CommandContext(ctx, "curl", "-X", "POST", "http://localhost:9091/-/reload")
	output, err := reloadCmd.CombinedOutput()
	
	if err != nil {
		// Si l'API ne r√©pond pas, red√©marrer le conteneur
		return nahs.dockerRestartAction(ctx, service, tracker)
	}
	
	log.Printf("Prometheus config reloaded: %s", string(output))
	time.Sleep(5 * time.Second)
	return nil
}

// Conditions de r√©cup√©ration

// conditionConsecutiveFailuresBelow retourne une condition bas√©e sur le nombre d'√©checs cons√©cutifs
func (nahs *NeuralAutoHealingSystem) conditionConsecutiveFailuresBelow(threshold int) RecoveryCondition {
	return func(service string, tracker *ServiceFailureTracker) bool {
		return tracker.ConsecutiveFailures < threshold
	}
}

// conditionConsecutiveFailuresAbove retourne une condition bas√©e sur le nombre d'√©checs cons√©cutifs
func (nahs *NeuralAutoHealingSystem) conditionConsecutiveFailuresAbove(threshold int) RecoveryCondition {
	return func(service string, tracker *ServiceFailureTracker) bool {
		return tracker.ConsecutiveFailures > threshold
	}
}

// GetServiceFailureStatus retourne le statut de failure d'un service
func (nahs *NeuralAutoHealingSystem) GetServiceFailureStatus(serviceName string) (*ServiceFailureTracker, bool) {
	nahs.mutex.RLock()
	defer nahs.mutex.RUnlock()
	
	tracker, exists := nahs.serviceFailures[serviceName]
	return tracker, exists
}

// GetAllFailureStatuses retourne tous les statuts de failure
func (nahs *NeuralAutoHealingSystem) GetAllFailureStatuses() map[string]*ServiceFailureTracker {
	nahs.mutex.RLock()
	defer nahs.mutex.RUnlock()
	
	// Copie pour √©viter les race conditions
	copy := make(map[string]*ServiceFailureTracker)
	for k, v := range nahs.serviceFailures {
		copy[k] = v
	}
	return copy
}

// Start d√©marre le syst√®me d'auto-healing
func (nahs *NeuralAutoHealingSystem) Start(ctx context.Context) error {
	log.Println("üöÄ Starting Neural Auto-Healing System...")
	
	nahs.isActive = true
	
	// D√©marrer la surveillance continue
	go nahs.startContinuousMonitoring(ctx)
	
	log.Println("‚úÖ Neural Auto-Healing System started")
	return nil
}

// Stop arr√™te le syst√®me d'auto-healing
func (nahs *NeuralAutoHealingSystem) Stop() error {
	log.Println("üõë Stopping Neural Auto-Healing System...")
	
	nahs.isActive = false
	
	log.Println("‚úÖ Neural Auto-Healing System stopped")
	return nil
}

// startContinuousMonitoring surveille en continu les services
func (nahs *NeuralAutoHealingSystem) startContinuousMonitoring(ctx context.Context) {
	ticker := time.NewTicker(60 * time.Second) // V√©rifier toutes les minutes
	defer ticker.Stop()
	
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			if nahs.isActive {
				nahs.checkServiceHealth(ctx)
			}
		}
	}
}

// checkServiceHealth v√©rifie la sant√© des services et d√©clenche l'auto-healing si n√©cessaire
func (nahs *NeuralAutoHealingSystem) checkServiceHealth(ctx context.Context) {
	// Cette m√©thode sera impl√©ment√©e avec la logique de v√©rification de sant√©
	// et de d√©clenchement d'auto-healing selon les besoins
}
