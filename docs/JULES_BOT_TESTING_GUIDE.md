# Testing Guide for Jules Bot Manual Review System

## Overview
This guide provides instructions for testing the Jules Bot manual review and approval system to ensure all components work correctly.

## ğŸ§ª Test Scenarios

### 1. Quality Assessment Testing

#### High Quality Contribution (Score â‰¥80)
**Setup:**
```bash
# Create a high-quality test contribution
git checkout -b jules-google/test-high-quality
echo "console.log('High quality feature');" > src/feature.js
echo "# Feature Documentation" > docs/feature.md
git add .
git commit -m "feat: add high quality feature with documentation"
git push origin jules-google/test-high-quality
```

**Expected Results:**
- âœ… Quality score â‰¥80
- âœ… Fast-track review recommended
- âœ… Minimal reviewers assigned
- âœ… Quality gates pass

#### Medium Quality Contribution (Score 60-79)
**Setup:**
```bash
# Create a medium-quality test contribution
git checkout -b jules-google/test-medium-quality
for i in {1..8}; do
  echo "console.log('File $i');" > src/file$i.js
done
git add .
git commit -m "add multiple files"
git push origin jules-google/test-medium-quality
```

**Expected Results:**
- ğŸŸ¡ Quality score 60-79
- ğŸŸ¡ Standard review required
- ğŸŸ¡ Multiple reviewers assigned
- âš ï¸ Manual review flagged

#### Low Quality Contribution (Score <60)
**Setup:**
```bash
# Create a low-quality test contribution
git checkout -b jules-google/test-low-quality
for i in {1..15}; do
  echo "console.log('File $i');" > src/file$i.js
done
echo "password=secret123" > config.env
git add .
git commit -m "wip"
git commit -m "temp fix"
git commit -m "debug"
git push origin jules-google/test-low-quality
```

**Expected Results:**
- ğŸ”´ Quality score <60
- ğŸ”´ Enhanced review required
- ğŸš¨ Security issues flagged
- ğŸ‘¥ Senior reviewers assigned

### 2. Review Command Testing

#### Test Approval Command
```bash
# In PR comment:
@jules-bot approve

# Expected:
# âœ… PR approved and merged to dev
# âœ… Success notification posted
# âœ… Branch cleaned up
```

#### Test Change Request
```bash
# In PR comment:
@jules-bot request-changes "Please add unit tests for the new feature"

# Expected:
# ğŸ”„ Change request recorded
# ğŸ”„ Feedback notification sent
# ğŸ”„ PR status updated
```

#### Test Rejection
```bash
# In PR comment:
@jules-bot close "This change conflicts with our architecture guidelines"

# Expected:
# âŒ PR closed
# âŒ Rejection reason logged
# âŒ Branch marked for cleanup
```

### 3. Permission Testing

#### Valid Reviewer
```bash
# Test with user having write access
@jules-bot approve

# Expected:
# âœ… Command processed
# âœ… Actions executed
```

#### Invalid Reviewer
```bash
# Test with user having read-only access
@jules-bot approve

# Expected:
# âŒ Permission denied message
# âŒ No actions taken
```

### 4. Integration Testing

#### Full Workflow Test
1. **Bot Contribution Creation**
   ```bash
   # Simulate Jules Bot creating contribution
   git checkout -b jules-google/auto-20241201-143022
   echo "// Jules Bot generated code" > src/bot-feature.js
   git add .
   git commit -m "feat: Jules Bot auto-generated feature"
   git push origin jules-google/auto-20241201-143022
   ```

2. **PR Creation**
   ```bash
   # Create PR to dev branch
   gh pr create --title "Jules Bot: Auto-generated feature" \
                --body "Automated contribution from Jules Bot" \
                --base dev \
                --head jules-google/auto-20241201-143022
   ```

3. **Quality Assessment**
   - Monitor workflow execution
   - Verify quality score calculation
   - Check reviewer assignment

4. **Manual Review**
   ```bash
   # Review and approve
   @jules-bot approve
   ```

5. **Integration Verification**
   ```bash
   # Verify merge to dev
   git checkout dev
   git pull origin dev
   git log --oneline -5
   ```

## ğŸ” Monitoring & Validation

### Quality Gates Validation
```yaml
# Check these metrics in quality report
file_count_check: â‰¤10 files preferred
file_size_check: no files >1MB
security_scan: no sensitive content
commit_quality: â‰¤5 commits, good messages
config_safety: safe configuration changes
documentation: adequate docs for code changes
```

### Review Process Validation
```yaml
# Verify these workflow steps
quality_assessment: automated scoring works
reviewer_assignment: correct reviewers notified
command_parsing: bot commands recognized
permission_validation: access control enforced
integration_process: merge to dev successful
notification_system: all stakeholders informed
```

## ğŸš¨ Error Scenarios

### Test Error Handling

#### Invalid PR Target
```bash
# Create PR to wrong branch
gh pr create --base main --head jules-google/test-branch

# Expected:
# âŒ Workflow should not trigger
# âŒ No quality assessment run
```

#### Merge Conflicts
```bash
# Create conflicting changes
git checkout dev
echo "conflicting change" > src/feature.js
git add . && git commit -m "conflict setup"
git push origin dev

# Then try to approve PR with conflicts
@jules-bot approve

# Expected:
# âš ï¸ Merge failure notification
# ğŸ”§ Manual intervention required
```

#### Security Issues
```bash
# Create PR with sensitive content
echo "api_key=sk_live_abc123xyz" > .env
git add . && git commit -m "add config"

# Expected:
# ğŸš¨ Security scan failure
# ğŸ”´ Low quality score
# ğŸ›‘ Enhanced review required
```

## ğŸ“Š Performance Testing

### Load Testing
```bash
# Test multiple concurrent PRs
for i in {1..5}; do
  git checkout -b jules-google/test-load-$i
  echo "Test $i" > test$i.js
  git add . && git commit -m "test $i"
  git push origin jules-google/test-load-$i &
done

# Expected:
# âœ… All PRs processed
# âœ… No workflow conflicts
# âœ… Consistent quality assessment
```

### Timeout Testing
```bash
# Test review SLA enforcement
# Create PR and wait beyond timeout threshold

# Expected:
# â° Escalation notifications
# ğŸ“§ Admin alerts triggered
# ğŸ“Š Metrics updated
```

## ğŸ“‹ Test Checklist

### Pre-deployment Testing
- [ ] Quality assessment algorithm accuracy
- [ ] Review command parsing reliability
- [ ] Permission system security
- [ ] Branch management automation
- [ ] Notification delivery
- [ ] Error handling robustness
- [ ] Performance under load
- [ ] Integration with existing systems

### Post-deployment Monitoring
- [ ] Quality score distribution trends
- [ ] Review turnaround times
- [ ] Approval/rejection rates
- [ ] System error frequency
- [ ] User satisfaction feedback
- [ ] Security incident tracking
- [ ] Performance metrics

## ğŸ”§ Troubleshooting

### Common Issues

#### Workflow Not Triggering
```bash
# Check branch naming
git branch --list "jules-google/*"

# Verify PR target
gh pr view --json baseRefName

# Check file patterns
git diff --name-only origin/dev...HEAD
```

#### Quality Assessment Errors
```bash
# Check Python dependencies
pip list | grep -E "(PyYAML|requests)"

# Verify git history
git log --oneline origin/dev...HEAD

# Test file access
ls -la .github/jules-config.yml
```

#### Permission Errors
```bash
# Check user permissions
gh api repos/:owner/:repo/collaborators/:username/permission

# Verify team membership
gh api user/teams
```

### Debug Commands
```bash
# Enable workflow debugging
# Add this to workflow environment:
ACTIONS_STEP_DEBUG: true
ACTIONS_RUNNER_DEBUG: true

# Check workflow logs
gh run list --workflow=jules-review-approval.yml
gh run view <run-id> --log
```

## ğŸ“ˆ Success Metrics

### Key Performance Indicators
- **Review Speed**: Average time from PR creation to approval
- **Quality Accuracy**: Correlation between automated score and human assessment
- **Approval Rate**: Percentage of contributions approved vs rejected
- **Error Rate**: Frequency of system failures or incorrect assessments
- **User Satisfaction**: Feedback from developers using the system

### Target Thresholds
```yaml
review_speed:
  fast_track: <4 hours
  standard: <24 hours
  enhanced: <48 hours

quality_accuracy: >90%
approval_rate: 70-80%
error_rate: <2%
user_satisfaction: >4.0/5.0
```

---

## ğŸ¯ Next Steps After Testing

1. **Gather Test Results** - Document all test outcomes
2. **Performance Tuning** - Adjust thresholds based on results
3. **User Training** - Create training materials for reviewers
4. **Production Rollout** - Gradual deployment with monitoring
5. **Continuous Improvement** - Regular assessment and optimization

---

*Test Guide Version: 1.0 | Last Updated: December 2024*
