Help on module python:

NAME
    python

DESCRIPTION
    pygments.lexers.python
    ~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Python and related languages.

    :copyright: Copyright 2006-2024 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.

CLASSES
    pip._vendor.pygments.lexer.DelegatingLexer(pip._vendor.pygments.lexer.Lexer)
        PythonConsoleLexer
    pip._vendor.pygments.lexer.RegexLexer(pip._vendor.pygments.lexer.Lexer)
        CythonLexer
        DgLexer
        Python2Lexer
        Python2TracebackLexer
        PythonLexer
            NumPyLexer
        PythonTracebackLexer

    class CythonLexer(pip._vendor.pygments.lexer.RegexLexer)
     |  CythonLexer(*args, **kwds)
     |
     |  For Pyrex and Cython source code.
     |
     |  Method resolution order:
     |      CythonLexer
     |      pip._vendor.pygments.lexer.RegexLexer
     |      pip._vendor.pygments.lexer.Lexer
     |      builtins.object
     |
     |  Data and other attributes defined here:
     |
     |  aliases = ['cython', 'pyx', 'pyrex']
     |
     |  filenames = ['*.pyx', '*.pxd', '*.pxi']
     |
     |  mimetypes = ['text/x-cython', 'application/x-cython']
     |
     |  name = 'Cython'
     |
     |  tokens = {'backtick': [('`.*?`', Token.Literal.String.Backtick)], 'bui...
     |
     |  url = 'https://cython.org'
     |
     |  version_added = '1.1'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  get_tokens_unprocessed(self, text, stack=('root',))
     |      Split ``text`` into (tokentype, text) pairs.
     |
     |      ``stack`` is the initial stack (default: ``['root']``)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  flags = re.MULTILINE
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  alias_filenames = []
     |
     |  priority = 0

    class DgLexer(pip._vendor.pygments.lexer.RegexLexer)
     |  DgLexer(*args, **kwds)
     |
     |  Lexer for dg,
     |  a functional and object-oriented programming language
     |  running on the CPython 3 VM.
     |
     |  Method resolution order:
     |      DgLexer
     |      pip._vendor.pygments.lexer.RegexLexer
     |      pip._vendor.pygments.lexer.Lexer
     |      builtins.object
     |
     |  Data and other attributes defined here:
     |
     |  aliases = ['dg']
     |
     |  filenames = ['*.dg']
     |
     |  mimetypes = ['text/x-dg']
     |
     |  name = 'dg'
     |
     |  tokens = {'dqs': [('"', Token.Literal.String, '#pop')], 'root': [(r'\s...
     |
     |  url = 'http://pyos.github.io/dg'
     |
     |  version_added = '1.6'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  get_tokens_unprocessed(self, text, stack=('root',))
     |      Split ``text`` into (tokentype, text) pairs.
     |
     |      ``stack`` is the initial stack (default: ``['root']``)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  flags = re.MULTILINE
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  alias_filenames = []
     |
     |  priority = 0

    class NumPyLexer(PythonLexer)
     |  NumPyLexer(*args, **kwds)
     |
     |  A Python lexer recognizing Numerical Python builtins.
     |
     |  Method resolution order:
     |      NumPyLexer
     |      PythonLexer
     |      pip._vendor.pygments.lexer.RegexLexer
     |      pip._vendor.pygments.lexer.Lexer
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  get_tokens_unprocessed(self, text)
     |      Split ``text`` into (tokentype, text) pairs.
     |
     |      ``stack`` is the initial stack (default: ``['root']``)
     |
     |  ----------------------------------------------------------------------
     |  Static methods defined here:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  EXTRA_KEYWORDS = {'abs', 'absolute', 'accumulate', 'add', 'alen', 'all...
     |
     |  aliases = ['numpy']
     |
     |  filenames = []
     |
     |  mimetypes = []
     |
     |  name = 'NumPy'
     |
     |  url = 'https://numpy.org/'
     |
     |  version_added = '0.10'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from PythonLexer:
     |
     |  fstring_rules(ttype)
     |
     |  innerstring_rules(ttype)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from PythonLexer:
     |
     |  tokens = {'builtins': [(<pip._vendor.pygments.lexer.words object>, Tok...
     |
     |  uni_name = '[A-Z_a-zªµºÀ-ÖØ-öø-\u02c1ˆ-\u02d1\u02e0-\u02e4\u02ec\u02ee\u0370-\u0374\u0376-\u0377\u037b-\u037d\u037f\u0386\u0388-\u038a\u038c\u038e-\u03a1\u03a3-...\U0001ee6a\U0001ee6c-\U0001ee72\U0001ee74-\U0001ee77...
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  flags = re.MULTILINE
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  alias_filenames = []
     |
     |  priority = 0

    class Python2Lexer(pip._vendor.pygments.lexer.RegexLexer)
     |  Python2Lexer(*args, **kwds)
     |
     |  For Python 2.x source code.
     |
     |  .. versionchanged:: 2.5
     |     This class has been renamed from ``PythonLexer``.  ``PythonLexer`` now
     |     refers to the Python 3 variant.  File name patterns like ``*.py`` have
     |     been moved to Python 3 as well.
     |
     |  Method resolution order:
     |      Python2Lexer
     |      pip._vendor.pygments.lexer.RegexLexer
     |      pip._vendor.pygments.lexer.Lexer
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  innerstring_rules(ttype)
     |
     |  ----------------------------------------------------------------------
     |  Static methods defined here:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  aliases = ['python2', 'py2']
     |
     |  filenames = []
     |
     |  mimetypes = ['text/x-python2', 'application/x-python2']
     |
     |  name = 'Python 2.x'
     |
     |  tokens = {'backtick': [('`.*?`', Token.Literal.String.Backtick)], 'bui...
     |
     |  url = 'https://www.python.org'
     |
     |  version_added = ''
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  get_tokens_unprocessed(self, text, stack=('root',))
     |      Split ``text`` into (tokentype, text) pairs.
     |
     |      ``stack`` is the initial stack (default: ``['root']``)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  flags = re.MULTILINE
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  alias_filenames = []
     |
     |  priority = 0

    class Python2TracebackLexer(pip._vendor.pygments.lexer.RegexLexer)
     |  Python2TracebackLexer(*args, **kwds)
     |
     |  For Python tracebacks.
     |
     |  .. versionchanged:: 2.5
     |     This class has been renamed from ``PythonTracebackLexer``.
     |     ``PythonTracebackLexer`` now refers to the Python 3 variant.
     |
     |  Method resolution order:
     |      Python2TracebackLexer
     |      pip._vendor.pygments.lexer.RegexLexer
     |      pip._vendor.pygments.lexer.Lexer
     |      builtins.object
     |
     |  Data and other attributes defined here:
     |
     |  aliases = ['py2tb']
     |
     |  filenames = ['*.py2tb']
     |
     |  mimetypes = ['text/x-python2-traceback']
     |
     |  name = 'Python 2.x Traceback'
     |
     |  tokens = {'intb': [(r'^(  File )("[^"]+")(, line )(\d+)(, in )(.+)(\n)...
     |
     |  url = 'https://python.org'
     |
     |  version_added = '0.7'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  get_tokens_unprocessed(self, text, stack=('root',))
     |      Split ``text`` into (tokentype, text) pairs.
     |
     |      ``stack`` is the initial stack (default: ``['root']``)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  flags = re.MULTILINE
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  alias_filenames = []
     |
     |  priority = 0

    class PythonConsoleLexer(pip._vendor.pygments.lexer.DelegatingLexer)
     |  PythonConsoleLexer(**options)
     |
     |  For Python console output or doctests, such as:
     |
     |  .. sourcecode:: pycon
     |
     |      >>> a = 'foo'
     |      >>> print(a)
     |      foo
     |      >>> 1 / 0
     |      Traceback (most recent call last):
     |        File "<stdin>", line 1, in <module>
     |      ZeroDivisionError: integer division or modulo by zero
     |
     |  Additional options:
     |
     |  `python3`
     |      Use Python 3 lexer for code.  Default is ``True``.
     |
     |      .. versionadded:: 1.0
     |      .. versionchanged:: 2.5
     |         Now defaults to ``True``.
     |
     |  Method resolution order:
     |      PythonConsoleLexer
     |      pip._vendor.pygments.lexer.DelegatingLexer
     |      pip._vendor.pygments.lexer.Lexer
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  aliases = ['pycon', 'python-console']
     |
     |  mimetypes = ['text/x-python-doctest']
     |
     |  name = 'Python console session'
     |
     |  url = 'https://python.org'
     |
     |  version_added = ''
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.DelegatingLexer:
     |
     |  get_tokens_unprocessed(self, text)
     |      This method should process the text and return an iterable of
     |      ``(index, tokentype, value)`` tuples where ``index`` is the starting
     |      position of the token within the input text.
     |
     |      It must be overridden by subclasses. It is recommended to
     |      implement it as a generator to maximize effectiveness.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  alias_filenames = []
     |
     |  filenames = []
     |
     |  priority = 0

    class PythonLexer(pip._vendor.pygments.lexer.RegexLexer)
     |  PythonLexer(*args, **kwds)
     |
     |  For Python source code (version 3.x).
     |
     |  .. versionchanged:: 2.5
     |     This is now the default ``PythonLexer``.  It is still available as the
     |     alias ``Python3Lexer``.
     |
     |  Method resolution order:
     |      PythonLexer
     |      pip._vendor.pygments.lexer.RegexLexer
     |      pip._vendor.pygments.lexer.Lexer
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  fstring_rules(ttype)
     |
     |  innerstring_rules(ttype)
     |
     |  ----------------------------------------------------------------------
     |  Static methods defined here:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  aliases = ['python', 'py', 'sage', 'python3', 'py3', 'bazel', 'starlar...
     |
     |  filenames = ['*.py', '*.pyw', '*.pyi', '*.jy', '*.sage', '*.sc', 'SCon...
     |
     |  mimetypes = ['text/x-python', 'application/x-python', 'text/x-python3'...
     |
     |  name = 'Python'
     |
     |  tokens = {'builtins': [(<pip._vendor.pygments.lexer.words object>, Tok...
     |
     |  uni_name = '[A-Z_a-zªµºÀ-ÖØ-öø-\u02c1ˆ-\u02d1\u02e0-\u02e4\u02ec\u02ee\u0370-\u0374\u0376-\u0377\u037b-\u037d\u037f\u0386\u0388-\u038a\u038c\u038e-\u03a1\u03a3-...\U0001ee6a\U0001ee6c-\U0001ee72\U0001ee74-\U0001ee77...
     |
     |  url = 'https://www.python.org'
     |
     |  version_added = '0.10'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  get_tokens_unprocessed(self, text, stack=('root',))
     |      Split ``text`` into (tokentype, text) pairs.
     |
     |      ``stack`` is the initial stack (default: ``['root']``)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  flags = re.MULTILINE
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  alias_filenames = []
     |
     |  priority = 0

    class PythonTracebackLexer(pip._vendor.pygments.lexer.RegexLexer)
     |  PythonTracebackLexer(*args, **kwds)
     |
     |  For Python 3.x tracebacks, with support for chained exceptions.
     |
     |  .. versionchanged:: 2.5
     |     This is now the default ``PythonTracebackLexer``.  It is still available
     |     as the alias ``Python3TracebackLexer``.
     |
     |  Method resolution order:
     |      PythonTracebackLexer
     |      pip._vendor.pygments.lexer.RegexLexer
     |      pip._vendor.pygments.lexer.Lexer
     |      builtins.object
     |
     |  Data and other attributes defined here:
     |
     |  aliases = ['pytb', 'py3tb']
     |
     |  filenames = ['*.pytb', '*.py3tb']
     |
     |  mimetypes = ['text/x-python-traceback', 'text/x-python3-traceback']
     |
     |  name = 'Python Traceback'
     |
     |  tokens = {'intb': [(r'^(  File )("[^"]+")(, line )(\d+)(, in )(.+)(\n)...
     |
     |  url = 'https://python.org'
     |
     |  version_added = '1.0'
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  get_tokens_unprocessed(self, text, stack=('root',))
     |      Split ``text`` into (tokentype, text) pairs.
     |
     |      ``stack`` is the initial stack (default: ``['root']``)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.RegexLexer:
     |
     |  flags = re.MULTILINE
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from pip._vendor.pygments.lexer.Lexer:
     |
     |  alias_filenames = []
     |
     |  priority = 0

DATA
    __all__ = ['PythonLexer', 'PythonConsoleLexer', 'PythonTracebackLexer'...

FILE
    d:\do\web\n8n_tests\projets\email_sender_1\projet\venv\lib\site-packages\pip\_vendor\pygments\lexers\python.py


