Help on module lexer:

NAME
    lexer

DESCRIPTION
    pygments.lexer
    ~~~~~~~~~~~~~~

    Base lexer classes.

    :copyright: Copyright 2006-2024 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.

CLASSES
    builtins.object
        Lexer
            DelegatingLexer
            RegexLexer
                ExtendedRegexLexer
        LexerContext
        default
    builtins.str(builtins.object)
        include
    pip._vendor.pygments.util.Future(builtins.object)
        words

    class DelegatingLexer(Lexer)
     |  DelegatingLexer(_root_lexer, _language_lexer, _needle=Token.Other, **options)
     |
     |  This lexer takes two lexer as arguments. A root lexer and
     |  a language lexer. First everything is scanned using the language
     |  lexer, afterwards all ``Other`` tokens are lexed using the root
     |  lexer.
     |
     |  The lexers from the ``template`` lexer package use this base lexer.
     |
     |  Method resolution order:
     |      DelegatingLexer
     |      Lexer
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, _root_lexer, _language_lexer, _needle=Token.Other, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  get_tokens_unprocessed(self, text)
     |      This method should process the text and return an iterable of
     |      ``(index, tokentype, value)`` tuples where ``index`` is the starting
     |      position of the token within the input text.
     |
     |      It must be overridden by subclasses. It is recommended to
     |      implement it as a generator to maximize effectiveness.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from Lexer:
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from Lexer:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from Lexer:
     |
     |  alias_filenames = []
     |
     |  aliases = []
     |
     |  filenames = []
     |
     |  mimetypes = []
     |
     |  name = None
     |
     |  priority = 0
     |
     |  url = None
     |
     |  version_added = None

    class ExtendedRegexLexer(RegexLexer)
     |  ExtendedRegexLexer(*args, **kwds)
     |
     |  A RegexLexer that uses a context object to store its state.
     |
     |  Method resolution order:
     |      ExtendedRegexLexer
     |      RegexLexer
     |      Lexer
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  get_tokens_unprocessed(self, text=None, context=None)
     |      Split ``text`` into (tokentype, text) pairs.
     |      If ``context`` is given, use this lexer context instead.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from RegexLexer:
     |
     |  flags = re.MULTILINE
     |
     |  tokens = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from Lexer:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from Lexer:
     |
     |  alias_filenames = []
     |
     |  aliases = []
     |
     |  filenames = []
     |
     |  mimetypes = []
     |
     |  name = None
     |
     |  priority = 0
     |
     |  url = None
     |
     |  version_added = None

    class Lexer(builtins.object)
     |  Lexer(**options)
     |
     |  Lexer for a specific language.
     |
     |  See also :doc:`lexerdevelopment`, a high-level guide to writing
     |  lexers.
     |
     |  Lexer classes have attributes used for choosing the most appropriate
     |  lexer based on various criteria.
     |
     |  .. autoattribute:: name
     |     :no-value:
     |  .. autoattribute:: aliases
     |     :no-value:
     |  .. autoattribute:: filenames
     |     :no-value:
     |  .. autoattribute:: alias_filenames
     |  .. autoattribute:: mimetypes
     |     :no-value:
     |  .. autoattribute:: priority
     |
     |  Lexers included in Pygments should have two additional attributes:
     |
     |  .. autoattribute:: url
     |     :no-value:
     |  .. autoattribute:: version_added
     |     :no-value:
     |
     |  Lexers included in Pygments may have additional attributes:
     |
     |  .. autoattribute:: _example
     |     :no-value:
     |
     |  You can pass options to the constructor. The basic options recognized
     |  by all lexers and processed by the base `Lexer` class are:
     |
     |  ``stripnl``
     |      Strip leading and trailing newlines from the input (default: True).
     |  ``stripall``
     |      Strip all leading and trailing whitespace from the input
     |      (default: False).
     |  ``ensurenl``
     |      Make sure that the input ends with a newline (default: True).  This
     |      is required for some lexers that consume input linewise.
     |
     |      .. versionadded:: 1.3
     |
     |  ``tabsize``
     |      If given and greater than 0, expand tabs in the input (default: 0).
     |  ``encoding``
     |      If given, must be an encoding name. This encoding will be used to
     |      convert the input string to Unicode, if it is not already a Unicode
     |      string (default: ``'guess'``, which uses a simple UTF-8 / Locale /
     |      Latin1 detection.  Can also be ``'chardet'`` to use the chardet
     |      library, if it is installed.
     |  ``inencoding``
     |      Overrides the ``encoding`` if given.
     |
     |  Methods defined here:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  get_tokens_unprocessed(self, text)
     |      This method should process the text and return an iterable of
     |      ``(index, tokentype, value)`` tuples where ``index`` is the starting
     |      position of the token within the input text.
     |
     |      It must be overridden by subclasses. It is recommended to
     |      implement it as a generator to maximize effectiveness.
     |
     |  ----------------------------------------------------------------------
     |  Static methods defined here:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  alias_filenames = []
     |
     |  aliases = []
     |
     |  filenames = []
     |
     |  mimetypes = []
     |
     |  name = None
     |
     |  priority = 0
     |
     |  url = None
     |
     |  version_added = None

    class LexerContext(builtins.object)
     |  LexerContext(text, pos, stack=None, end=None)
     |
     |  A helper object that holds lexer position data.
     |
     |  Methods defined here:
     |
     |  __init__(self, text, pos, stack=None, end=None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object

    class RegexLexer(Lexer)
     |  RegexLexer(*args, **kwds)
     |
     |  Base for simple stateful regular expression-based lexers.
     |  Simplifies the lexing process so that you need only
     |  provide a list of states and regular expressions.
     |
     |  Method resolution order:
     |      RegexLexer
     |      Lexer
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  get_tokens_unprocessed(self, text, stack=('root',))
     |      Split ``text`` into (tokentype, text) pairs.
     |
     |      ``stack`` is the initial stack (default: ``['root']``)
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  flags = re.MULTILINE
     |
     |  tokens = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from Lexer:
     |
     |  __init__(self, **options)
     |      This constructor takes arbitrary options as keyword arguments.
     |      Every subclass must first process its own options and then call
     |      the `Lexer` constructor, since it processes the basic
     |      options like `stripnl`.
     |
     |      An example looks like this:
     |
     |      .. sourcecode:: python
     |
     |         def __init__(self, **options):
     |             self.compress = options.get('compress', '')
     |             Lexer.__init__(self, **options)
     |
     |      As these options must all be specifiable as strings (due to the
     |      command line usage), there are various utility functions
     |      available to help with that, see `Utilities`_.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  add_filter(self, filter_, **options)
     |      Add a new stream filter to this lexer.
     |
     |  get_tokens(self, text, unfiltered=False)
     |      This method is the basic interface of a lexer. It is called by
     |      the `highlight()` function. It must process the text and return an
     |      iterable of ``(tokentype, value)`` pairs from `text`.
     |
     |      Normally, you don't need to override this method. The default
     |      implementation processes the options recognized by all lexers
     |      (`stripnl`, `stripall` and so on), and then yields all tokens
     |      from `get_tokens_unprocessed()`, with the ``index`` dropped.
     |
     |      If `unfiltered` is set to `True`, the filtering mechanism is
     |      bypassed even if filters are defined.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from Lexer:
     |
     |  analyse_text = text_analyse(text) from pip._vendor.pygments.util.make_analysator.<locals>
     |      A static method which is called for lexer guessing.
     |
     |      It should analyse the text and return a float in the range
     |      from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
     |      will not be selected as the most probable one, if it returns
     |      ``1.0``, it will be selected immediately.  This is used by
     |      `guess_lexer`.
     |
     |      The `LexerMeta` metaclass automatically wraps this function so
     |      that it works like a static method (no ``self`` or ``cls``
     |      parameter) and the return value is automatically converted to
     |      `float`. If the return value is an object that is boolean `False`
     |      it's the same as if the return values was ``0.0``.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Lexer:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from Lexer:
     |
     |  alias_filenames = []
     |
     |  aliases = []
     |
     |  filenames = []
     |
     |  mimetypes = []
     |
     |  name = None
     |
     |  priority = 0
     |
     |  url = None
     |
     |  version_added = None

    class default(builtins.object)
     |  default(state)
     |
     |  Indicates a state or state action (e.g. #pop) to apply.
     |  For example default('#pop') is equivalent to ('', Token, '#pop')
     |  Note that state tuples may be used as well.
     |
     |  .. versionadded:: 2.0
     |
     |  Methods defined here:
     |
     |  __init__(self, state)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object

    class include(builtins.str)
     |  Indicates that a state should include rules from another state.
     |
     |  Method resolution order:
     |      include
     |      builtins.str
     |      builtins.object
     |
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from builtins.str:
     |
     |  __add__(self, value, /)
     |      Return self+value.
     |
     |  __contains__(self, key, /)
     |      Return bool(key in self).
     |
     |  __eq__(self, value, /)
     |      Return self==value.
     |
     |  __format__(self, format_spec, /)
     |      Return a formatted version of the string as described by format_spec.
     |
     |  __ge__(self, value, /)
     |      Return self>=value.
     |
     |  __getitem__(self, key, /)
     |      Return self[key].
     |
     |  __getnewargs__(...)
     |
     |  __gt__(self, value, /)
     |      Return self>value.
     |
     |  __hash__(self, /)
     |      Return hash(self).
     |
     |  __iter__(self, /)
     |      Implement iter(self).
     |
     |  __le__(self, value, /)
     |      Return self<=value.
     |
     |  __len__(self, /)
     |      Return len(self).
     |
     |  __lt__(self, value, /)
     |      Return self<value.
     |
     |  __mod__(self, value, /)
     |      Return self%value.
     |
     |  __mul__(self, value, /)
     |      Return self*value.
     |
     |  __ne__(self, value, /)
     |      Return self!=value.
     |
     |  __repr__(self, /)
     |      Return repr(self).
     |
     |  __rmod__(self, value, /)
     |      Return value%self.
     |
     |  __rmul__(self, value, /)
     |      Return value*self.
     |
     |  __sizeof__(self, /)
     |      Return the size of the string in memory, in bytes.
     |
     |  __str__(self, /)
     |      Return str(self).
     |
     |  capitalize(self, /)
     |      Return a capitalized version of the string.
     |
     |      More specifically, make the first character have upper case and the rest lower
     |      case.
     |
     |  casefold(self, /)
     |      Return a version of the string suitable for caseless comparisons.
     |
     |  center(self, width, fillchar=' ', /)
     |      Return a centered string of length width.
     |
     |      Padding is done using the specified fill character (default is a space).
     |
     |  count(...)
     |      S.count(sub[, start[, end]]) -> int
     |
     |      Return the number of non-overlapping occurrences of substring sub in
     |      string S[start:end].  Optional arguments start and end are
     |      interpreted as in slice notation.
     |
     |  encode(self, /, encoding='utf-8', errors='strict')
     |      Encode the string using the codec registered for encoding.
     |
     |      encoding
     |        The encoding in which to encode the string.
     |      errors
     |        The error handling scheme to use for encoding errors.
     |        The default is 'strict' meaning that encoding errors raise a
     |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and
     |        'xmlcharrefreplace' as well as any other name registered with
     |        codecs.register_error that can handle UnicodeEncodeErrors.
     |
     |  endswith(...)
     |      S.endswith(suffix[, start[, end]]) -> bool
     |
     |      Return True if S ends with the specified suffix, False otherwise.
     |      With optional start, test S beginning at that position.
     |      With optional end, stop comparing S at that position.
     |      suffix can also be a tuple of strings to try.
     |
     |  expandtabs(self, /, tabsize=8)
     |      Return a copy where all tab characters are expanded using spaces.
     |
     |      If tabsize is not given, a tab size of 8 characters is assumed.
     |
     |  find(...)
     |      S.find(sub[, start[, end]]) -> int
     |
     |      Return the lowest index in S where substring sub is found,
     |      such that sub is contained within S[start:end].  Optional
     |      arguments start and end are interpreted as in slice notation.
     |
     |      Return -1 on failure.
     |
     |  format(...)
     |      S.format(*args, **kwargs) -> str
     |
     |      Return a formatted version of S, using substitutions from args and kwargs.
     |      The substitutions are identified by braces ('{' and '}').
     |
     |  format_map(...)
     |      S.format_map(mapping) -> str
     |
     |      Return a formatted version of S, using substitutions from mapping.
     |      The substitutions are identified by braces ('{' and '}').
     |
     |  index(...)
     |      S.index(sub[, start[, end]]) -> int
     |
     |      Return the lowest index in S where substring sub is found,
     |      such that sub is contained within S[start:end].  Optional
     |      arguments start and end are interpreted as in slice notation.
     |
     |      Raises ValueError when the substring is not found.
     |
     |  isalnum(self, /)
     |      Return True if the string is an alpha-numeric string, False otherwise.
     |
     |      A string is alpha-numeric if all characters in the string are alpha-numeric and
     |      there is at least one character in the string.
     |
     |  isalpha(self, /)
     |      Return True if the string is an alphabetic string, False otherwise.
     |
     |      A string is alphabetic if all characters in the string are alphabetic and there
     |      is at least one character in the string.
     |
     |  isascii(self, /)
     |      Return True if all characters in the string are ASCII, False otherwise.
     |
     |      ASCII characters have code points in the range U+0000-U+007F.
     |      Empty string is ASCII too.
     |
     |  isdecimal(self, /)
     |      Return True if the string is a decimal string, False otherwise.
     |
     |      A string is a decimal string if all characters in the string are decimal and
     |      there is at least one character in the string.
     |
     |  isdigit(self, /)
     |      Return True if the string is a digit string, False otherwise.
     |
     |      A string is a digit string if all characters in the string are digits and there
     |      is at least one character in the string.
     |
     |  isidentifier(self, /)
     |      Return True if the string is a valid Python identifier, False otherwise.
     |
     |      Call keyword.iskeyword(s) to test whether string s is a reserved identifier,
     |      such as "def" or "class".
     |
     |  islower(self, /)
     |      Return True if the string is a lowercase string, False otherwise.
     |
     |      A string is lowercase if all cased characters in the string are lowercase and
     |      there is at least one cased character in the string.
     |
     |  isnumeric(self, /)
     |      Return True if the string is a numeric string, False otherwise.
     |
     |      A string is numeric if all characters in the string are numeric and there is at
     |      least one character in the string.
     |
     |  isprintable(self, /)
     |      Return True if the string is printable, False otherwise.
     |
     |      A string is printable if all of its characters are considered printable in
     |      repr() or if it is empty.
     |
     |  isspace(self, /)
     |      Return True if the string is a whitespace string, False otherwise.
     |
     |      A string is whitespace if all characters in the string are whitespace and there
     |      is at least one character in the string.
     |
     |  istitle(self, /)
     |      Return True if the string is a title-cased string, False otherwise.
     |
     |      In a title-cased string, upper- and title-case characters may only
     |      follow uncased characters and lowercase characters only cased ones.
     |
     |  isupper(self, /)
     |      Return True if the string is an uppercase string, False otherwise.
     |
     |      A string is uppercase if all cased characters in the string are uppercase and
     |      there is at least one cased character in the string.
     |
     |  join(self, iterable, /)
     |      Concatenate any number of strings.
     |
     |      The string whose method is called is inserted in between each given string.
     |      The result is returned as a new string.
     |
     |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'
     |
     |  ljust(self, width, fillchar=' ', /)
     |      Return a left-justified string of length width.
     |
     |      Padding is done using the specified fill character (default is a space).
     |
     |  lower(self, /)
     |      Return a copy of the string converted to lowercase.
     |
     |  lstrip(self, chars=None, /)
     |      Return a copy of the string with leading whitespace removed.
     |
     |      If chars is given and not None, remove characters in chars instead.
     |
     |  partition(self, sep, /)
     |      Partition the string into three parts using the given separator.
     |
     |      This will search for the separator in the string.  If the separator is found,
     |      returns a 3-tuple containing the part before the separator, the separator
     |      itself, and the part after it.
     |
     |      If the separator is not found, returns a 3-tuple containing the original string
     |      and two empty strings.
     |
     |  removeprefix(self, prefix, /)
     |      Return a str with the given prefix string removed if present.
     |
     |      If the string starts with the prefix string, return string[len(prefix):].
     |      Otherwise, return a copy of the original string.
     |
     |  removesuffix(self, suffix, /)
     |      Return a str with the given suffix string removed if present.
     |
     |      If the string ends with the suffix string and that suffix is not empty,
     |      return string[:-len(suffix)]. Otherwise, return a copy of the original
     |      string.
     |
     |  replace(self, old, new, count=-1, /)
     |      Return a copy with all occurrences of substring old replaced by new.
     |
     |        count
     |          Maximum number of occurrences to replace.
     |          -1 (the default value) means replace all occurrences.
     |
     |      If the optional argument count is given, only the first count occurrences are
     |      replaced.
     |
     |  rfind(...)
     |      S.rfind(sub[, start[, end]]) -> int
     |
     |      Return the highest index in S where substring sub is found,
     |      such that sub is contained within S[start:end].  Optional
     |      arguments start and end are interpreted as in slice notation.
     |
     |      Return -1 on failure.
     |
     |  rindex(...)
     |      S.rindex(sub[, start[, end]]) -> int
     |
     |      Return the highest index in S where substring sub is found,
     |      such that sub is contained within S[start:end].  Optional
     |      arguments start and end are interpreted as in slice notation.
     |
     |      Raises ValueError when the substring is not found.
     |
     |  rjust(self, width, fillchar=' ', /)
     |      Return a right-justified string of length width.
     |
     |      Padding is done using the specified fill character (default is a space).
     |
     |  rpartition(self, sep, /)
     |      Partition the string into three parts using the given separator.
     |
     |      This will search for the separator in the string, starting at the end. If
     |      the separator is found, returns a 3-tuple containing the part before the
     |      separator, the separator itself, and the part after it.
     |
     |      If the separator is not found, returns a 3-tuple containing two empty strings
     |      and the original string.
     |
     |  rsplit(self, /, sep=None, maxsplit=-1)
     |      Return a list of the substrings in the string, using sep as the separator string.
     |
     |        sep
     |          The separator used to split the string.
     |
     |          When set to None (the default value), will split on any whitespace
     |          character (including \n \r \t \f and spaces) and will discard
     |          empty strings from the result.
     |        maxsplit
     |          Maximum number of splits.
     |          -1 (the default value) means no limit.
     |
     |      Splitting starts at the end of the string and works to the front.
     |
     |  rstrip(self, chars=None, /)
     |      Return a copy of the string with trailing whitespace removed.
     |
     |      If chars is given and not None, remove characters in chars instead.
     |
     |  split(self, /, sep=None, maxsplit=-1)
     |      Return a list of the substrings in the string, using sep as the separator string.
     |
     |        sep
     |          The separator used to split the string.
     |
     |          When set to None (the default value), will split on any whitespace
     |          character (including \n \r \t \f and spaces) and will discard
     |          empty strings from the result.
     |        maxsplit
     |          Maximum number of splits.
     |          -1 (the default value) means no limit.
     |
     |      Splitting starts at the front of the string and works to the end.
     |
     |      Note, str.split() is mainly useful for data that has been intentionally
     |      delimited.  With natural text that includes punctuation, consider using
     |      the regular expression module.
     |
     |  splitlines(self, /, keepends=False)
     |      Return a list of the lines in the string, breaking at line boundaries.
     |
     |      Line breaks are not included in the resulting list unless keepends is given and
     |      true.
     |
     |  startswith(...)
     |      S.startswith(prefix[, start[, end]]) -> bool
     |
     |      Return True if S starts with the specified prefix, False otherwise.
     |      With optional start, test S beginning at that position.
     |      With optional end, stop comparing S at that position.
     |      prefix can also be a tuple of strings to try.
     |
     |  strip(self, chars=None, /)
     |      Return a copy of the string with leading and trailing whitespace removed.
     |
     |      If chars is given and not None, remove characters in chars instead.
     |
     |  swapcase(self, /)
     |      Convert uppercase characters to lowercase and lowercase characters to uppercase.
     |
     |  title(self, /)
     |      Return a version of the string where each word is titlecased.
     |
     |      More specifically, words start with uppercased characters and all remaining
     |      cased characters have lower case.
     |
     |  translate(self, table, /)
     |      Replace each character in the string using the given translation table.
     |
     |        table
     |          Translation table, which must be a mapping of Unicode ordinals to
     |          Unicode ordinals, strings, or None.
     |
     |      The table must implement lookup/indexing via __getitem__, for instance a
     |      dictionary or list.  If this operation raises LookupError, the character is
     |      left untouched.  Characters mapped to None are deleted.
     |
     |  upper(self, /)
     |      Return a copy of the string converted to uppercase.
     |
     |  zfill(self, width, /)
     |      Pad a numeric string with zeros on the left, to fill a field of the given width.
     |
     |      The string is never truncated.
     |
     |  ----------------------------------------------------------------------
     |  Static methods inherited from builtins.str:
     |
     |  __new__(*args, **kwargs) class method of builtins.str
     |      Create and return a new object.  See help(type) for accurate signature.
     |
     |  maketrans(...)
     |      Return a translation table usable for str.translate().
     |
     |      If there is only one argument, it must be a dictionary mapping Unicode
     |      ordinals (integers) or characters to Unicode ordinals, strings or None.
     |      Character keys will be then converted to ordinals.
     |      If there are two arguments, they must be strings of equal length, and
     |      in the resulting dictionary, each character in x will be mapped to the
     |      character at the same position in y. If there is a third argument, it
     |      must be a string, whose characters will be mapped to None in the result.

    class words(pip._vendor.pygments.util.Future)
     |  words(words, prefix='', suffix='')
     |
     |  Indicates a list of literal words that is transformed into an optimized
     |  regex that matches any of the words.
     |
     |  .. versionadded:: 2.0
     |
     |  Method resolution order:
     |      words
     |      pip._vendor.pygments.util.Future
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, words, prefix='', suffix='')
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  get(self)
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from pip._vendor.pygments.util.Future:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object

FUNCTIONS
    bygroups(*args)
        Callback that yields multiple actions for each group in the match.

    using(_other, **kwargs)
        Callback that processes the match with a different lexer.

        The keyword arguments are forwarded to the lexer, except `state` which
        is handled separately.

        `state` specifies the state that the new lexer will start in, and can
        be an enumerable such as ('root', 'inline', 'string') or a simple
        string which is assumed to be on top of the root state.

        Note: For that to work, `_other` must not be an `ExtendedRegexLexer`.

DATA
    __all__ = ['Lexer', 'RegexLexer', 'ExtendedRegexLexer', 'DelegatingLex...
    inherit = inherit
    line_re = re.compile('.*?\n')
    this = <lexer._This object>

FILE
    d:\do\web\n8n_tests\projets\email_sender_1\projet\venv\lib\site-packages\pip\_vendor\pygments\lexer.py


